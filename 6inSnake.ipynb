{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import pygame\n",
    "from pygame.math import Vector3\n",
    "from OpenGL.GL import *\n",
    "from OpenGL.GLU import *\n",
    "from OpenGL.GLUT import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "GRID_SIZE = 10\n",
    "DIMENSIONS = 6\n",
    "N_ACTIONS = 2 * DIMENSIONS\n",
    "N_AGENTS = 3\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.995\n",
    "TARGET_UPDATE = 100\n",
    "LEARNING_RATE = 0.0005\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Snake environment\n",
    "class Snake6D:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.snakes = [deque([[random.randint(0, GRID_SIZE-1) for _ in range(DIMENSIONS)]]) for _ in range(N_AGENTS)]\n",
    "        self.food = self.generate_food()\n",
    "        self.scores = [0] * N_AGENTS\n",
    "        return self.get_state()\n",
    "\n",
    "    def generate_food(self):\n",
    "        while True:\n",
    "            food = [random.randint(0, GRID_SIZE-1) for _ in range(DIMENSIONS)]\n",
    "            if not any(food in snake for snake in self.snakes):\n",
    "                return food\n",
    "\n",
    "    def get_state(self):\n",
    "        state = []\n",
    "        for snake in self.snakes:\n",
    "            head = snake[0]\n",
    "            state.extend(head)\n",
    "        state.extend(self.food)\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = [0] * N_AGENTS\n",
    "        done = [False] * N_AGENTS  # Done condition per agent\n",
    "\n",
    "        for i, action in enumerate(actions):\n",
    "            direction = [0] * DIMENSIONS\n",
    "            direction[action // 2] = 1 if action % 2 == 0 else -1\n",
    "            \n",
    "            new_head = [\n",
    "                (self.snakes[i][0][j] + direction[j]) % GRID_SIZE\n",
    "                for j in range(DIMENSIONS)\n",
    "            ]\n",
    "\n",
    "            # Check if new head is in the snake's body (self-collision)\n",
    "            if new_head in self.snakes[i]:\n",
    "                rewards[i] = -10  # Higher penalty for self-collision\n",
    "                done[i] = True\n",
    "            else:\n",
    "                self.snakes[i].appendleft(new_head)\n",
    "                if new_head == self.food:\n",
    "                    self.scores[i] += 1\n",
    "                    rewards[i] = 10  # Reward for eating food\n",
    "                    self.food = self.generate_food()\n",
    "                else:\n",
    "                    self.snakes[i].pop()\n",
    "\n",
    "            # Boundary check (handle collisions properly with wrapping)\n",
    "            if any(coord == 0 or coord == GRID_SIZE-1 for coord in new_head):\n",
    "                rewards[i] = -5  # Penalty for hitting boundaries (if applicable)\n",
    "\n",
    "        # If any agent reaches a score of 50, end the game\n",
    "        if max(self.scores) >= 50:\n",
    "            done = [True] * N_AGENTS\n",
    "\n",
    "        return self.get_state(), rewards, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Agent\n",
    "class Agent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON_START\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(torch.tensor(state, device=self.device)).max(0)[1].item()\n",
    "        else:\n",
    "            return random.randrange(N_ACTIONS)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def update_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, device=self.device)\n",
    "        actions = torch.tensor(actions, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, device=self.device)\n",
    "        next_states = torch.tensor(next_states, device=self.device)\n",
    "\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        next_q = self.target_net(next_states).max(1)[0].detach()\n",
    "        target_q = rewards + (GAMMA * next_q)\n",
    "\n",
    "        loss = nn.functional.smooth_l1_loss(current_q, target_q.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization class\n",
    "class Visualizer:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.display = (800, 600)\n",
    "        pygame.display.set_mode(self.display, pygame.DOUBLEBUF | pygame.OPENGL)\n",
    "        gluPerspective(45, (self.display[0] / self.display[1]), 0.1, 50.0)\n",
    "        glTranslatef(0.0, 0.0, -30)\n",
    "        self.colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]\n",
    "\n",
    "    def draw_cube(self, position, color):\n",
    "        x, y, z = position[:3]\n",
    "        glPushMatrix()\n",
    "        glTranslatef(x - GRID_SIZE/2, y - GRID_SIZE/2, z - GRID_SIZE/2)\n",
    "        glScale(0.5, 0.5, 0.5)\n",
    "        glColor3fv(color)\n",
    "        \n",
    "        vertices = [\n",
    "            (1, -1, -1), (1, 1, -1), (-1, 1, -1), (-1, -1, -1),\n",
    "            (1, -1, 1), (1, 1, 1), (-1, -1, 1), (-1, 1, 1)\n",
    "        ]\n",
    "        edges = [\n",
    "            (0,1), (0,3), (0,4), (2,1), (2,3), (2,7),\n",
    "            (6,3), (6,4), (6,7), (5,1), (5,4), (5,7)\n",
    "        ]\n",
    "        \n",
    "        glBegin(GL_LINES)\n",
    "        for edge in edges:\n",
    "            for vertex in edge:\n",
    "                glVertex3fv(vertices[vertex])\n",
    "        glEnd()\n",
    "        \n",
    "        glPopMatrix()\n",
    "\n",
    "    def draw_enclosed_cube(self):\n",
    "        glColor3f(1, 1, 1)  # White color for the enclosing cube\n",
    "        glPushMatrix()\n",
    "        glScale(GRID_SIZE/2, GRID_SIZE/2, GRID_SIZE/2)\n",
    "        glutWireCube(2)\n",
    "        glPopMatrix()\n",
    "\n",
    "    def draw_scene(self, env):\n",
    "        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "        glRotatef(1, 3, 1, 1)\n",
    "\n",
    "        self.draw_enclosed_cube()\n",
    "\n",
    "        # Draw snakes\n",
    "        for i, snake in enumerate(env.snakes):\n",
    "            for segment in snake:\n",
    "                self.draw_cube(segment, self.colors[i])\n",
    "\n",
    "        # Draw food\n",
    "        self.draw_cube(env.food, (1, 1, 1))  # White color for food\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(episodes):\n",
    "    env = Snake6D()\n",
    "    state_dim = DIMENSIONS * (N_AGENTS + 1)  # State includes all snake heads and food position\n",
    "    agents = [Agent(state_dim, N_ACTIONS) for _ in range(N_AGENTS)]\n",
    "    visualizer = Visualizer()\n",
    "\n",
    "    scores_history = [[] for _ in range(N_AGENTS)]\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards = [0] * N_AGENTS\n",
    "        done = [False] * N_AGENTS\n",
    "\n",
    "        while not all(done):\n",
    "            actions = [agents[i].select_action(state) for i in range(N_AGENTS)]\n",
    "            next_state, rewards, done = env.step(actions)\n",
    "\n",
    "            for i in range(N_AGENTS):\n",
    "                agents[i].store_transition(state, actions[i], rewards[i], next_state)\n",
    "                agents[i].update_model()\n",
    "                total_rewards[i] += rewards[i]\n",
    "\n",
    "            state = next_state\n",
    "            visualizer.draw_scene(env)\n",
    "        \n",
    "        for i in range(N_AGENTS):\n",
    "            scores_history[i].append(total_rewards[i])\n",
    "            agents[i].update_epsilon()\n",
    "\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target_network()\n",
    "\n",
    "        print(f\"Episode {episode}: {total_rewards}\")\n",
    "\n",
    "    return agents, scores_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "def test_agents(agents, episodes=10):\n",
    "    env = Snake6D()\n",
    "    visualizer = Visualizer()\n",
    "    test_scores_history = [[] for _ in range(N_AGENTS)]\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards = [0] * N_AGENTS\n",
    "        done = [False] * N_AGENTS\n",
    "\n",
    "        while not all(done):\n",
    "            # No exploration, only use the learned policy\n",
    "            actions = [agents[i].select_action(state) for i in range(N_AGENTS)]\n",
    "            next_state, rewards, done = env.step(actions)\n",
    "\n",
    "            for i in range(N_AGENTS):\n",
    "                total_rewards[i] += rewards[i]\n",
    "\n",
    "            state = next_state\n",
    "            visualizer.draw_scene(env)\n",
    "\n",
    "        for i in range(N_AGENTS):\n",
    "            test_scores_history[i].append(total_rewards[i])\n",
    "\n",
    "        print(f\"Test Episode {episode}: {total_rewards}\")\n",
    "\n",
    "    return test_scores_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:19:47.642 Python[74598:11137409] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-04 19:19:47.642 Python[74598:11137409] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "/var/folders/tf/p7wv1ltx4qd3ck7qrjqnmjhr0000gn/T/ipykernel_74598/1250882978.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  states = torch.tensor(states, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "# Plot the scores\n",
    "def plot_scores(scores_history, test_scores_history=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(N_AGENTS):\n",
    "        plt.plot(scores_history[i], label=f\"Agent {i+1} (Training)\")\n",
    "    if test_scores_history:\n",
    "        for i in range(N_AGENTS):\n",
    "            plt.plot(test_scores_history[i], label=f\"Agent {i+1} (Testing)\", linestyle='--')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"6D Snake Game - Agent Scores Over Time\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust plot limits to zoom in on the relevant score range\n",
    "    plt.ylim([-20, 60])  # Adjust based on observed scores\n",
    "\n",
    "    plt.savefig(\"training_results.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Train the agents\n",
    "episodes = 1000\n",
    "agents, scores_history = train(episodes)\n",
    "\n",
    "# Test the agents after training\n",
    "test_scores_history = test_agents(agents, episodes=10)\n",
    "\n",
    "# Plot both training and testing scores\n",
    "plot_scores(scores_history, test_scores_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
